import streamlit as st
from openai import OpenAI
import datetime

# 1. Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Î£ÎµÎ»Î¯Î´Î±Ï‚
st.set_page_config(page_title="Maqueen Robotics Lab", page_icon="ğŸ¤–", layout="wide")

# 2. Î£ÏÎ½Î´ÎµÏƒÎ· Î¼Îµ Ï„Î¿ API Ï„Î·Ï‚ Groq (Î£Ï…Î¼Î²Î±Ï„ÏŒ Î¼Îµ OpenAI format)
try:
    # Î›Î®ÏˆÎ· API Key Î±Ï€ÏŒ Ï„Î± Secrets Ï„Î¿Ï… Streamlit Cloud
    api_key_secret = st.secrets["GROQ_API_KEY"]
    client = OpenAI(
        base_url="https://api.groq.com/openai/v1",
        api_key=api_key_secret
    )
except Exception as e:
    st.error("âŒ Î¤Î¿ API Key (GROQ_API_KEY) Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ„Î± Secrets!")
    st.stop()

# 3. Sidebar - Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î³Î¹Î± Ï„Î¿Î½ ÎšÎ±Î¸Î·Î³Î·Ï„Î®
st.sidebar.title("âš™ï¸ Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Admin")
if st.sidebar.checkbox("Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î•Ï€Î¹Î»Î¿Î³ÏÎ½ ÎšÎ±Ï„ÎµÎ²Î¬ÏƒÎ¼Î±Ï„Î¿Ï‚"):
    try:
        with open("research_logs.txt", "r", encoding="utf-8") as f:
            st.sidebar.download_button(
                label="ğŸ“¥ ÎšÎ±Ï„Î­Î²Î±ÏƒÎ¼Î± Logs (TXT)",
                data=f,
                file_name=f"robotics_logs_{datetime.date.today()}.txt",
                mime="text/plain"
            )
    except FileNotFoundError:
        st.sidebar.warning("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±ÎºÏŒÎ¼Î± ÎºÎ±Ï„Î±Î³ÏÎ±Ï†Î­Ï‚.")

# 4. ÎšÏÏÎ¹Î¿ Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½ ÎœÎ±Î¸Î·Ï„Î®
st.title("ğŸ¤– Maqueen Micro:bit AI Assistant")
st.info("Î ÎµÏÎ¯Î³ÏÎ±ÏˆÎµ Ï„Î¹ Î¸Î­Î»ÎµÎ¹Ï‚ Î½Î± ÎºÎ¬Î½ÎµÎ¹ Ï„Î¿ ÏÎ¿Î¼Ï€ÏŒÏ„ Maqueen ÎºÎ±Î¹ Ï€Î¬ÏÎµ Ï„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ± ÏƒÎµ Python!")

student_id = st.text_input("ID ÎœÎ±Î¸Î·Ï„Î®:", "Guest")
user_prompt = st.text_area("Î ÎµÏÎ¯Î³ÏÎ±ÏˆÎµ Ï„Î·Î½ Î±Ï€Î¿ÏƒÏ„Î¿Î»Î® (Ï€.Ï‡. 'Î‘Ï€Î¿Ï†Ï…Î³Î® ÎµÎ¼Ï€Î¿Î´Î¯Ï‰Î½ Î¼Îµ Ï…Ï€ÎµÏÎ®Ï‡Î¿Ï…Ï‚'):")

# 5. Î•ÎºÏ„Î­Î»ÎµÏƒÎ· ÎºÎ±Î¹ Î Î±ÏÎ±Î³Ï‰Î³Î® ÎšÏÎ´Î¹ÎºÎ±
if st.button("ğŸš€ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎšÏÎ´Î¹ÎºÎ± & ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î®"):
    if user_prompt:
        with st.spinner('â³ Î¤Î¿ AI Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ±...'):
            try:
                # ÎšÎ»Î®ÏƒÎ· Ï„Î¿Ï… Llama 3 Î¼Îµ System Prompt Î³Î¹Î± Maqueen
                response = client.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    messages=[
                        {
                            "role": "system", 
                            "content": (
                                "Î•Î¯ÏƒÎ±Î¹ Î­Î½Î±Ï‚ Î­Î¼Ï€ÎµÎ¹ÏÎ¿Ï‚ ÎºÎ±Î¸Î·Î³Î·Ï„Î®Ï‚ ÏÎ¿Î¼Ï€Î¿Ï„Î¹ÎºÎ®Ï‚ Maqueen. "
                                "Î‘Ï€Î±Î½Ï„Î¬Ï‚ Ï€Î¬Î½Ï„Î± ÏƒÏ„Î± Î•Î»Î»Î·Î½Î¹ÎºÎ¬. "
                                "Î”Î¯Î½ÎµÎ¹Ï‚ Ï€Î¬Î½Ï„Î± Î¿Î»ÏŒÎºÎ»Î·ÏÎ¿ Ï„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ± MicroPython Î³Î¹Î± Ï„Î¿ Micro:bit. "
                                "Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ ÏƒÏ‰ÏƒÏ„Î­Ï‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ ÎºÎ±Î¹ pins Î³Î¹Î± Ï„Î¿ Maqueen. "
                                "Î•Î¾Î®Î³Î·ÏƒÎµ Î±Î½Î±Î»Ï…Ï„Î¹ÎºÎ¬ Ï„Î¹ ÎºÎ¬Î½ÎµÎ¹ ÎºÎ¬Î¸Îµ Ï„Î¼Î®Î¼Î± Ï„Î¿Ï… ÎºÏÎ´Î¹ÎºÎ±."
                            )
                        },
                        {"role": "user", "content": user_prompt}
                    ]
                )
                
                answer = response.choices[0].message.content
                
                # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î¿Ï‚ ÏƒÏ„Î·Î½ Î¿Î¸ÏŒÎ½Î·
                st.subheader("ğŸ“ ÎŸ ÎšÏÎ´Î¹ÎºÎ±Ï‚ Python:")
                st.markdown(answer)
                
                # 6. Î‘Ï€Î¿
