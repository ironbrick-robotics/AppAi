import streamlit as st
from openai import OpenAI
import datetime
import requests

# --- Î•Î¡Î•Î¥ÎÎ—Î¤Î™ÎšÎŸ Î Î•Î¡Î™Î’Î‘Î›Î›ÎŸÎ IRONBRICK ---
# Î¥Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ·: [Î¤Î¿ ÎŸÎ½Î¿Î¼Î±Ï„ÎµÏ€ÏÎ½Ï…Î¼ÏŒ Î£Î¿Ï…]

st.set_page_config(page_title="ironbrick Research IDE", layout="wide")

# Custom CSS
st.markdown("<style>header {visibility: hidden;} .stExpander { border: 2px solid #00a0dc; border-radius: 8px; }</style>", unsafe_allow_html=True)

# Î£ÏÎ½Î´ÎµÏƒÎ·
try:
    client = OpenAI(base_url="https://api.groq.com/openai/v1", api_key=st.secrets["GROQ_API_KEY"])
    DB_URL = st.secrets["GSHEET_URL"]
except:
    st.error("Connection Error.")

# --- RESEARCH LOGIC ---
MY_CODING_LOGIC = (
    "Analyze the student's prompt and categorize it: "
    "L1: Simple natural language, L2: Parameters/Values, L3: Logic/Loops, "
    "L4: Technical Terminology (Huskylens, RGB, etc), L5: Debugging. "
    "Return only the label (e.g., L1)."
)

# Î‘ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Session
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "last_output" not in st.session_state:
    st.session_state.last_output = ""

# Layout
tab_ide, tab_logs = st.tabs(["ğŸ’» IDE", "ğŸ“Š Data Access"])

with tab_ide:
    col1, col2 = st.columns([1, 1])
    
    with col1:
        # ÎšÎ¿Ï…Î¼Ï€Î¯ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚
        if st.button("ğŸ—‘ï¸ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î™ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï"):
            st.session_state.chat_history = []
            st.session_state.last_output = ""
            st.rerun()

        with st.form("input_form"):
            student_id = st.text_input("Student Code:", "S01")
            # Î‘Î»Î»Î±Î³Î® Task names
            mode = st.radio("Î¤ÏÏ€Î¿Ï‚ Î•Î½Î­ÏÎ³ÎµÎ¹Î±Ï‚:", ["ÎÎ­Î± Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î±", "Î”Î¹Î¿ÏÎ¸Ï‰ÏƒÎ· ÎµÎ½Ï„Î¿Î»Î®Ï‚"], horizontal=True)
            user_input = st.text_area("Î ÎµÏÎ¹Î³ÏÎ¬ÏˆÏ„Îµ Ï„Î·Î½ Î±Ï€Î¿ÏƒÏ„Î¿Î»Î® ÏƒÏ„Î¿ Maqueen:", height=150)
            btn = st.form_submit_button("Î•ÎºÏ„Î­Î»ÎµÏƒÎ·")

    with col2:
        if btn and user_input:
            # Î‘Î½ ÎµÎ¯Î½Î±Î¹ ÎÎ­Î± Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î±, ÎºÎ±Î¸Î±ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ Ï€ÏÎ¹Î½ Ï„Î¿ Î½Î­Î¿ prompt
            if mode == "ÎÎ­Î± Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î±":
                st.session_state.chat_history = []
            
            st.session_state.chat_history.append({"role": "user", "content": user_input})
            
            with st.spinner('Î‘Î½Î¬Î»Ï…ÏƒÎ·...'):
                # 1. ÎšÎ±Ï„Î¬Ï„Î±Î¾Î· L1-L5
                analysis = client.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    messages=[{"role": "system", "content": MY_CODING_LOGIC}, {"role": "user", "content": user_input}]
                )
                current_level = analysis.choices[0].message.content.strip()

                # 2. Î Î±ÏÎ±Î³Ï‰Î³Î® ÎšÏÎ´Î¹ÎºÎ± (Maqueen & Huskylens focus)
                maqueen_sys = (
                    "You are a Maqueen Robot expert. Generate ONLY clean code for micro:bit Maqueen. "
                    "Support libraries: Maqueen, Huskylens, NeoPixel. "
                    "Return ONLY the code, no markdown, no comments, no greetings."
                )
                messages = [{"role": "system", "content": maqueen_sys}] + st.session_state.chat_history
                code_res = client.chat.completions.create(model="llama-3.3-70b-versatile", messages=messages)
                final_code = code_res.choices[0].message.content.strip()
                
                st.session_state.last_output = final_code
                st.markdown(f"**Î•Ï€Î¯Ï€ÎµÎ´Î¿: {current_level}**")
                st.code(final_code, language='python')

                # 3. Logging
                log_entry = {
                    "data": [{
                        "Timestamp": str(datetime.datetime.now()),
                        "Student_ID": student_id,
                        "Action": mode,
                        "Coding_Level": current_level,
                        "Prompt": user_input,
                        "Code": final_code.replace('"', "'")
                    }]
                }
                requests.post(DB_URL, json=log_entry)

    # Î•Ï€ÎµÎ¾Î®Î³Î·ÏƒÎ·
    if st.session_state.last_output:
        with st.expander("ğŸ’¡ Î•Ï€ÎµÎ¾Î®Î³Î·ÏƒÎ· ÎšÏÎ´Î¹ÎºÎ±"):
            exp_sys = "Î•Î¯ÏƒÎ±Î¹ ÎºÎ±Î¸Î·Î³Î·Ï„Î®Ï‚ ÏÎ¿Î¼Ï€Î¿Ï„Î¹ÎºÎ®Ï‚. Î•Î¾Î®Î³Î·ÏƒÎµ Ï„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ± Maqueen ÎœÎŸÎÎŸ ÏƒÏ„Î± Î•Î»Î»Î·Î½Î¹ÎºÎ¬ Î¼Îµ Î±Ï€Î»Î¬ Î»ÏŒÎ³Î¹Î±."
            explanation = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "system", "content": exp_sys}, {"role": "user", "content": st.session_state.last_output}]
            )
            st.write(explanation.choices[0].message.content)
